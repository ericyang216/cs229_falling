{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore if not windows\n",
    "import winsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_sample = pd.read_csv('samples/2015_nominal_samples_n1.csv')\n",
    "gray_sample = pd.read_csv('samples/2015_gray_samples_n1.csv')\n",
    "failure_sample = pd.read_csv('samples/2015_failure_samples_n1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_sample = nominal_sample.dropna(axis=0, thresh=20).dropna(axis=1)\n",
    "failure_sample = failure_sample.dropna(axis=0, thresh=20).dropna(axis=1)\n",
    "gray_sample    = gray_sample.dropna(axis=0, thresh=20).dropna(axis=1)\n",
    "gray_sample.failure = 1\n",
    "nominal_sample = nominal_sample.sample(frac=(failure_sample.shape[0]+gray_sample.shape[0])/nominal_sample.shape[0])\n",
    "data_curr = pd.concat([nominal_sample, failure_sample, gray_sample])\n",
    "data_curr = data_curr.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalized\n",
    "feature_columns = [ 'smart_1_normalized', 'smart_3_normalized', 'smart_4_normalized', 'smart_5_normalized',\n",
    "           'smart_7_normalized','smart_12_normalized', 'smart_194_normalized', \n",
    "           'smart_197_normalized', 'smart_198_normalized', 'smart_199_normalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw\n",
    "feature_columns = [ 'smart_1_raw', 'smart_3_raw', 'smart_4_raw', 'smart_5_raw',\n",
    "           'smart_7_raw','smart_12_raw', 'smart_194_raw', \n",
    "           'smart_197_raw', 'smart_198_raw', 'smart_199_raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 'None'\n",
    "train_size = .66\n",
    "\n",
    "if scale=='sk':\n",
    "    y_curr = data_curr['failure']\n",
    "    x_curr = data_curr[feature_columns]\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_curr, y_curr, train_size=train_size)\n",
    "\n",
    "    x_train = x_train.values\n",
    "    x_val = x_val.values\n",
    "    y_train = y_train.values\n",
    "    y_val = y_val.values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_val = scaler.transform(x_val)\n",
    "elif scale=='sk_manu':\n",
    "    y_curr = data_curr['failure']\n",
    "    x_train, x_val, y_train, y_val = train_test_split(data_curr, y_curr, train_size=train_size)\n",
    "\n",
    "    train_norm = pd.DataFrame()\n",
    "    test_norm = pd.DataFrame()\n",
    "    manufacturers = ['ST','Hi','WD','To','HG']\n",
    "    for man in manufacturers:\n",
    "        data_sub = x_train[[i[:2].lower()==man.lower() for i in x_train.model]]\n",
    "        test_sub = x_val[[i[:2].lower()==man.lower() for i in x_val.model]]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        data_sub = scaler.fit_transform(data_sub[feature_columns])\n",
    "        test_sub = scaler.transform(test_sub[feature_columns])\n",
    "        \n",
    "        train_norm = pd.concat([train_norm,pd.DataFrame(data_sub)])\n",
    "        test_norm = pd.concat([test_norm,pd.DataFrame(test_sub)])\n",
    "        \n",
    "        \n",
    "    x_train = train_norm.sample(frac=1)#[feature_columns]\n",
    "    x_val = test_norm.sample(frac=1)#[feature_columns]\n",
    "\n",
    "    x_train = x_train.values\n",
    "    x_val = x_val.values\n",
    "    y_train = y_train.values\n",
    "    y_val = y_val.values\n",
    "elif scale=='manu':\n",
    "    y_curr = data_curr['failure']\n",
    "    x_train, x_val, y_train, y_val = train_test_split(data_curr, y_curr, train_size=train_size)\n",
    "    \n",
    "    train_norm = pd.DataFrame()\n",
    "    test_norm = pd.DataFrame()\n",
    "    manufacturers = ['ST','Hi','WD','To','HG']\n",
    "    for man in manufacturers:    \n",
    "        data_sub = x_train[[i[:2].lower()==man.lower() for i in x_train.model]]\n",
    "        test_sub = x_val[[i[:2].lower()==man.lower() for i in x_val.model]]\n",
    "        max_ = data_sub.max()\n",
    "        del max_['date']\n",
    "        min_ = data_sub.min()\n",
    "        del min_['date']\n",
    "        for col in feature_columns:\n",
    "            range_ = max_[col] - min_[col]\n",
    "            if range_==0:\n",
    "                range_ = 1\n",
    "            data_sub[col] = (data_sub[col] - min_[col])/range_\n",
    "            test_sub[col] = (test_sub[col] - min_[col])/range_\n",
    "        train_norm = pd.concat([train_norm,data_sub])\n",
    "        test_norm = pd.concat([test_norm,test_sub])\n",
    "        \n",
    "    x_train = train_norm.sample(frac=1)[feature_columns]\n",
    "    x_val = test_norm.sample(frac=1)[feature_columns]\n",
    "\n",
    "    x_train = x_train.values\n",
    "    x_val = x_val.values\n",
    "    y_train = y_train.values\n",
    "    y_val = y_val.values\n",
    "    \n",
    "else:\n",
    "    y_curr = data_curr['failure']\n",
    "    x_curr = data_curr[feature_columns]\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_curr, y_curr, train_size=train_size)\n",
    "\n",
    "    x_train = x_train.values\n",
    "    x_val = x_val.values\n",
    "    y_train = y_train.values\n",
    "    y_val = y_val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** MODELS ***\n",
    "lr1 = LogisticRegression(solver='newton-cg')\n",
    "lr2 = LogisticRegression(solver='lbfgs')\n",
    "lr3 = LogisticRegression(solver='saga')\n",
    "lr4 = LogisticRegression(solver='sag')\n",
    "lr5 = LogisticRegression(solver='liblinear')\n",
    "\n",
    "svm_linear = svm.SVC(kernel='linear', gamma='scale', probability=True)\n",
    "svm_rbf = svm.SVC(kernel='rbf', gamma='scale', probability=True)\n",
    "svm_poly = svm.SVC(kernel='poly', gamma='scale', probability=True)\n",
    "svm_sig = svm.SVC(kernel='sigmoid', gamma='scale', probability=True)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc100 = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "boost1 = GradientBoostingClassifier(loss='deviance')\n",
    "boost2 = GradientBoostingClassifier(loss='exponential')\n",
    "\n",
    "gnb = GaussianNB()\n",
    "ada = AdaBoostClassifier(gnb, algorithm=\"SAMME\", n_estimators=200,learning_rate=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = {}\n",
    "#models = [lr1, lr2, lr3, lr4, lr5, svm_linear, svm_poly, rfc, rfc100, boost1, boost2, gnb, ada]\n",
    "models = [lr1, svm_poly, svm_linear, rfc, rfc100, boost1, boost2, gnb, ada]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1*(y_train>.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** RUN MODELS ***\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    pred_train = model.predict_proba(x_train)[:,1]\n",
    "    pred_val = model.predict_proba(x_val)[:,1]\n",
    "    \n",
    "    eval_v = helper.evaluation_metrics(pred_val, y_val)\n",
    "    eval_t = helper.evaluation_metrics(pred_train, y_train)\n",
    "    model_results[model] = {}\n",
    "    model_results[model]['train'] = eval_t\n",
    "    model_results[model]['val'] = eval_v\n",
    "    \n",
    "# *** RUN MODELS ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winsound.MessageBeep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_plot = [lr1, rfc, svm_poly, boost2, gnb, ada]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leg = []\n",
    "data = []\n",
    "for (i,model) in enumerate(models_plot):\n",
    "    leg.append(str(model).split('(')[0])\n",
    "    data.append(model_results[model]['val']['roc_auc'])\n",
    "    plt.bar(i, model_results[model]['val']['roc_auc'])\n",
    "plt.legend(leg,bbox_to_anchor=(0., 1.2, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.ylim([0,1])\n",
    "plt.title(\"AUC for Different Models\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leg = []\n",
    "data = []\n",
    "for (i,model) in enumerate(models_plot):\n",
    "    leg.append(str(model).split('(')[0])\n",
    "    data.append(model_results[model]['val']['f1'])\n",
    "    plt.bar(i, model_results[model]['val']['f1'])\n",
    "plt.legend(leg,bbox_to_anchor=(0., 1.2, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.ylim([0,1])\n",
    "plt.title(\"F1 for Different Models\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leg = []\n",
    "data = []\n",
    "for (i,model) in enumerate(models_plot):\n",
    "    leg.append(str(model).split('(')[0])\n",
    "    data.append(model_results[model]['val']['precision'])\n",
    "    plt.bar(i, model_results[model]['val']['precision'])\n",
    "plt.legend(leg,bbox_to_anchor=(0., 1.2, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.ylim([0,1])\n",
    "plt.title(\"Precision for Different Models\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leg = []\n",
    "data = []\n",
    "for (i,model) in enumerate(models_plot):\n",
    "    leg.append(str(model).split('(')[0])\n",
    "    data.append(model_results[model]['val']['recall'])\n",
    "    plt.bar(i, model_results[model]['val']['recall'])\n",
    "plt.legend(leg,bbox_to_anchor=(0., 1.2, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.ylim([0,1])\n",
    "plt.title(\"Recall for Different Models\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leg = []\n",
    "data = []\n",
    "for (i,model) in enumerate(models_plot):\n",
    "    leg.append(str(model).split('(')[0])\n",
    "    data.append(model_results[model]['val']['accuracy'])\n",
    "    plt.bar(i, model_results[model]['val']['accuracy'])\n",
    "plt.legend(leg,bbox_to_anchor=(0., 1.2, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.title(\"Accuracy for Different Models\")\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_t = []\n",
    "data_v = []\n",
    "for (i,model) in enumerate(models_plot):\n",
    "    data_t.append(model_results[model]['train']['roc_auc'])\n",
    "    data_v.append(model_results[model]['val']['roc_auc'])\n",
    "plt.bar([3*i for i in range(len(models_plot))], data_t)\n",
    "plt.bar([3*i+1 for i in range(len(models_plot))], data_v)\n",
    "plt.xticks(np.arange(0,3*len(models_plot),3), [str(m).split('(')[0] for m in models_plot],\n",
    "          rotation='vertical')\n",
    "plt.legend([\"Training\",\"Testing\"],bbox_to_anchor=(0., 1.2, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.title(\"AUC for Different Models at Train and Test Time\")\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_t = []\n",
    "data_v = []\n",
    "for (i,model) in enumerate(models_plot):\n",
    "    data_t.append(model_results[model]['train']['accuracy'])\n",
    "    data_v.append(model_results[model]['val']['accuracy'])\n",
    "plt.bar([3*i for i in range(len(models_plot))], data_t)\n",
    "plt.bar([3*i+1 for i in range(len(models_plot))], data_v)\n",
    "plt.xticks(np.arange(0,3*len(models_plot),3), [str(m).split('(')[0] for m in models_plot],\n",
    "          rotation='vertical')\n",
    "plt.legend([\"Training\",\"Testing\"],bbox_to_anchor=(0., 1.2, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.title(\"Accuracy for Different Models at Train and Test Time\")\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i,model) in enumerate(models_plot):\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
